# Technical Implementation Documentation

## 1. Application Architecture

The **Algorithm Benchmarker** is structured around a strict C# 12 / .NET 8 WPF architecture utilizing the **MVVM (Model-View-ViewModel)** design pattern. It enforces the **Open-Closed Principle (SOLID)**, meaning new algorithms and categories can be dropped into the execution layer without requiring any mutations to the core benchmarking engine or user interface.

### Key Modules & Responsibilities

1. **View Layer (`/Views`, `.xaml`)**
   - Purely graphical layouts binding to matching `ViewModels`. No code-behind business logic.
   - `BenchmarkView`: Handles configuration binding and Execution Mode states.
   - `ResultsView`: Integrates `LiveChartsCore` for dynamic charting and standard WPF `DataGrid` representations of empirical bounds.

2. **ViewModel Layer (`/ViewModels`)**
   - Built on `CommunityToolkit.Mvvm` (Source Generating AsyncRelayCommands and ObservableProperties).
   - Serves as the intersection between WPF bindings and the lower-level execution services.

3. **Algorithm Protocol (`IAlgorithm.cs`)**
   - The contract that guarantees auto-discovery via Reflection.
   - Core method: `Execute(object input)`.
   
4. **Benchmarking Engine (`/Services/BenchmarkRunner.cs`)**
   - The core coordinator. Receives dynamic inputs generated by the `InputGenerator`, isolates iterations within Garbage Collection clear commands (`GC.Collect()`), maps Stopwatch elapsed times, and logs allocation overhead tracking (`GC.GetAllocatedBytesForCurrentThread()`).

## 2. Advanced Profiling Engine Data Flow

The integration of the 11 Phase 2 research features leverages sophisticated runtime interception without violating core benchmarking validity. 

### A. Non-Disturbing Heuristic Capture
Standard profiling hooks often ruin algorithm execution timings due to invocation overhead. The system circumvents this utilizing `[ThreadStatic]` asynchronous tracing (`ExecutionTracer.cs`).

*   **Data Flow**: 
    1. UI sets `Config.EnableMicroTracer = true`.
    2. `BenchmarkRunner` triggers `ExecutionTracer.StartTracing()`.
    3. The executing Algorithm calls lightweight static hooks: `ExecutionTracer.Record(Comparison)`.
    4. Because the `List<OperationRecord>` inside the Tracer is `[ThreadStatic]`, there are no cross-thread lock contentions slowing down the algorithm.
    5. The Runner calls `ExecutionTracer.StopTracing()` and reads the metrics immediately prior to standardizing the `Dictionary<string, string> ExtendedMetrics` mapping table.

### B. Custom Execution Routes (Drag Race & Phase Transition)
While Standard Batches output arrays mapped sequentially over incrementally larger Data Inputs (producing smooth $O(N)$ curves), the advanced execution modes intercept the `BenchmarkRunner` entirely.

* **Drag Race Mode**: Handled by `DragRaceOrchestrator.cs`. 
    - Bypasses input scaling entirely. It instantiates a shared complex dataset.
    - Utilizes `System.Threading.Barrier` across spawned generic Tasks to perfectly synchronize thread start times up to the single JIT tick, ensuring environmental noise does not disproportionately punish concurrently executing algorithms.
    
* **Phase Transition Sweeper**: Handled by `AlgorithmicPhaseTransitionDetector.cs`.
    - It holds the fundamental `InputSize` static (e.g. `N=1000`) and instead initiates its primary scaling vector vertically against a structural constraint (e.g. Graph Density crossing from 0.01 to 1.0).
    - It maps the relative latency changes mathematically to discover inflection nodes (critical states where the algorithm degrades exponentially relative to the data topology).

## 3. Persistent Data Coupling
Results from the execution engine (including the newly appended stringified `ExtendedMetrics`) are pipelined directly to the `SQLiteRepository`, providing a localized file-system persistent database mapping. 
- This enables batch reloading, chart generation across multiple disjointed test runs, and cross-session export serialization logic (JSON/CSV handlers via `ExportService.cs`).

## 4. Key Design Decisions

- **Why Not Use BenchmarkDotNet?** 
  - `BenchmarkDotNet` compiles physical assemblies on the fly and spins up dedicated diagnostic environments. Integrating an educational/local research tool through that architecture isolates the dynamic state-sharing required by the WPF data bindings and dynamic input synthesizers. Retaining a native benchmarking loop allows customized Phase-Transition checks completely decoupled from process instantiation times.
- **Why Stringified Extended Metrics?**
  - Storing varied, dynamically scaling research outputs (from JIT cycles to Energy Joules) explicitly inside rigid SQLite schemas necessitates massive migration maps. Using a decoupled Key/Value dictionary normalized to an `ExtendedMetricsDisplay` variable cleanly allows limitless heuristic addition without breaking local state compatibility or WPF tabular bindings.
